"h_DistanceTraveled",
"h_FloorsStairsClimbed",
"h_HeartRateECG",
"h_SleepInfo",
"h_StepCount")
setOrder = c(health,devices,planets)
codeTypes = c(rep("Health Info",length(health)),
rep("Device Info",length(devices)),
rep("Planetary Info",length(planets)))
metadata <- as.data.frame(cbind(sets,codeTypes))
metadata$codeTypes <- as.character(metadata$codeTypes)
names(metadata) <- c("sets","CodeType")
#this isn't quite correct, so make it better
metadata$CodeType[metadata$sets %in% devices] <- "DeviceInfo"
metadata$CodeType[metadata$sets %in% planets] <- "PlanetaryInfo"
metadata$CodeType[metadata$sets %in% health] <- "HealthInfo"
library(UpSetR)
upset(df,nsets=50,nintersects = 329,mb.ratio=c(0.3,0.7),order.by = "freq",set.metadata = list(data = metadata, plots = list(list(type = "matrix_rows", column = "CodeType", assign = 10, colors = c(DeviceInfo = "gold", PlanetaryInfo = "orangered",HealthInfo = "deepskyblue")))))
upset(df,text.scale = c(1.3, 1.3, 1, 1, 2, 0.75),nsets=50,nintersects = 30,mb.ratio=c(0.3,0.7),order.by = "freq",set.metadata = list(data = metadata, plots = list(list(type = "matrix_rows", column = "CodeType", assign = 10, colors = c(DeviceInfo = "gold", PlanetaryInfo = "orangered",HealthInfo = "deepskyblue")))))
#df = read.csv("upset-alaul-questionnaire.csv")
cat("\014")
#df <- read.csv("~/Desktop/Smart-Watch Project-Alaul/Wear OS Project/Work Area/Survey Stat/Final_Survey_Stat/Up Set/upset-all-data-types.csv")
#df <- read.csv("~/Desktop/Smart-Watch Project-Alaul/Wear OS Project/Work Area/Survey Stat/Final_Survey_Stat/Up Set/upset-health-data-types.csv")
df <- read.csv("~/Desktop/Smart-Watch Project-Alaul/Wear OS Project/Work Area/Survey Stat/Final_Survey_Stat/Up Set/upset-weather-data-types.csv")
sets <- names(df)
# devices = c("d_Bluetooth",
#             "d_LocationName",
#             "d_PhoneBattery",
#             "d_WatchBattery",
#             "d_Wifi")
#
planets = c("w_Humidity",
"w_MoonPhase",
"w_SkyCondition",
"w_SunsetSunriseTime",
"w_Temperature",
"w_WindInfo")
setOrder = c(health,devices,planets)
codeTypes = c(rep("Health Info",length(health)),
rep("Device Info",length(devices)),
rep("Planetary Info",length(planets)))
metadata <- as.data.frame(cbind(sets,codeTypes))
metadata$codeTypes <- as.character(metadata$codeTypes)
names(metadata) <- c("sets","CodeType")
#this isn't quite correct, so make it better
metadata$CodeType[metadata$sets %in% devices] <- "DeviceInfo"
metadata$CodeType[metadata$sets %in% planets] <- "PlanetaryInfo"
metadata$CodeType[metadata$sets %in% health] <- "HealthInfo"
library(UpSetR)
upset(df,nsets=50,nintersects = 329,mb.ratio=c(0.3,0.7),order.by = "freq",set.metadata = list(data = metadata, plots = list(list(type = "matrix_rows", column = "CodeType", assign = 10, colors = c(DeviceInfo = "gold", PlanetaryInfo = "orangered",HealthInfo = "deepskyblue")))))
upset(df,text.scale = c(1.3, 1.3, 1, 1, 2, 0.75),nsets=50,nintersects = 30,mb.ratio=c(0.3,0.7),order.by = "freq",set.metadata = list(data = metadata, plots = list(list(type = "matrix_rows", column = "CodeType", assign = 10, colors = c(DeviceInfo = "gold", PlanetaryInfo = "orangered",HealthInfo = "deepskyblue")))))
setOrder = c(health,devices,planets)
codeTypes = c(rep("Health Info",length(health)),
rep("Device Info",length(devices)),
rep("Planetary Info",length(planets)))
metadata <- as.data.frame(cbind(sets,codeTypes))
metadata$codeTypes <- as.character(metadata$codeTypes)
names(metadata) <- c("sets","CodeType")
#this isn't quite correct, so make it better
metadata$CodeType[metadata$sets %in% devices] <- "DeviceInfo"
metadata$CodeType[metadata$sets %in% planets] <- "PlanetaryInfo"
metadata$CodeType[metadata$sets %in% health] <- "HealthInfo"
library(UpSetR)
upset(df,nsets=50,nintersects = 329,mb.ratio=c(0.3,0.7),order.by = "freq",set.metadata = list(data = metadata, plots = list(list(type = "matrix_rows", column = "CodeType", assign = 10, colors = c(DeviceInfo = "gold", PlanetaryInfo = "orangered",HealthInfo = "deepskyblue")))))
upset(df,text.scale = c(1.3, 1.3, 1, 1, 2, 0.75),nsets=50,nintersects = 30,mb.ratio=c(0.3,0.7),order.by = "freq",set.metadata = list(data = metadata, plots = list(list(type = "matrix_rows", column = "CodeType", assign = 10, colors = c(DeviceInfo = "gold", PlanetaryInfo = "orangered",HealthInfo = "deepskyblue")))))
#df = read.csv("upset-alaul-questionnaire.csv")
cat("\014")
#df <- read.csv("~/Desktop/Smart-Watch Project-Alaul/Wear OS Project/Work Area/Survey Stat/Final_Survey_Stat/Up Set/upset-all-data-types.csv")
#df <- read.csv("~/Desktop/Smart-Watch Project-Alaul/Wear OS Project/Work Area/Survey Stat/Final_Survey_Stat/Up Set/upset-health-data-types.csv")
#df <- read.csv("~/Desktop/Smart-Watch Project-Alaul/Wear OS Project/Work Area/Survey Stat/Final_Survey_Stat/Up Set/upset-weather-data-types.csv")
df <- read.csv("~/Desktop/Smart-Watch Project-Alaul/Wear OS Project/Work Area/Survey Stat/Final_Survey_Stat/Up Set/upset-device-data-types.csv")
sets <- names(df)
devices = c("d_Bluetooth",
"d_LocationName",
"d_PhoneBattery",
"d_WatchBattery",
"d_Wifi")
setOrder = c(health,devices,planets)
codeTypes = c(rep("Health Info",length(health)),
rep("Device Info",length(devices)),
rep("Planetary Info",length(planets)))
metadata <- as.data.frame(cbind(sets,codeTypes))
metadata$codeTypes <- as.character(metadata$codeTypes)
names(metadata) <- c("sets","CodeType")
#this isn't quite correct, so make it better
metadata$CodeType[metadata$sets %in% devices] <- "DeviceInfo"
metadata$CodeType[metadata$sets %in% planets] <- "PlanetaryInfo"
metadata$CodeType[metadata$sets %in% health] <- "HealthInfo"
library(UpSetR)
upset(df,nsets=50,nintersects = 329,mb.ratio=c(0.3,0.7),order.by = "freq",set.metadata = list(data = metadata, plots = list(list(type = "matrix_rows", column = "CodeType", assign = 10, colors = c(DeviceInfo = "gold", PlanetaryInfo = "orangered",HealthInfo = "deepskyblue")))))
upset(df,text.scale = c(1.3, 1.3, 1, 1, 2, 0.75),nsets=50,nintersects = 30,mb.ratio=c(0.3,0.7),order.by = "freq",set.metadata = list(data = metadata, plots = list(list(type = "matrix_rows", column = "CodeType", assign = 10, colors = c(DeviceInfo = "gold", PlanetaryInfo = "orangered",HealthInfo = "deepskyblue")))))
install.packages(c("fitdistrplus", "MPDiR", "quickpsy", "rstudioapi"))
library(plyr)
library(dplyr)
library(MPDiR)
library(quickpsy)
library(fitdistrplus)
library(ggplot2)
# Global values for our conditions
tasks <- c('T1','T2','T3')
sizes <- c('s1','s2','s3')
# set directory where script is
sourceDir <- dirname (rstudioapi::getActiveDocumentContext()$path)
defaultpath <- sourceDir
#remove(list = ls())
print(defaultpath)
setwd(defaultpath)
# get all log files
setwd("exp-data")
file_list <- list.files()
show(file_list)
if (exists ("allData")) { rm(allData) }
##we do the analysis once per task
for (task in tasks){
# basic loop that reads over participant files
# and computes normalized errors
print("Working on Task: ")
print(task)
print("-----------------------------------------")
allData <- NULL
for (file in file_list){
if(!grepl(task,file) | grepl("confidence",file) ){
#if the filename doesn't contain the current task short code or the word "confidence_survey" then don't take it
print("Skipping file")
print(file)
next
}
print("Analyzing file")
print(file)
setwd("~/Desktop/OnlineExperiment/AnalysisScripts/exp-data/")
dataFile <- read.csv(file, skip = 16, header = F)
# collect all trials in order to calculate error rate, keep non training trials (trial > 0)
tmp <- dataFile
# no training
tmp <- tmp [which(!grepl("practice",tmp$page_name,fixed=TRUE)),]
#convert the error, correct values
#convert the error, correct values
tmp$answer_numeric <- 0
#this way later we will calculate percept correct. If we want "percent wrong" switch the 0 and 1
tmp$answer_numeric[grep("correct",tmp$answer)] <- 1
#combine all files in one big one
if ( !exists("allData") ){
allData <- tmp
} else {
allData <- rbind(allData,tmp)
}
}
print(defaultpath)
setwd(defaultpath)
write.csv(allData, file=paste("~/Desktop/OnlineExperiment/AnalysisScripts/results/all_data-",task,".csv",sep=""))
}
library(plyr)
library(dplyr)
library(MPDiR)
library(quickpsy)
library(fitdistrplus)
library(ggplot2)
# Global values for our conditions
tasks <- c('T1','T2','T3')
sizes <- c('s1','s2','s3')
# set directory where script is
sourceDir <- dirname (rstudioapi::getActiveDocumentContext()$path)
defaultpath <- sourceDir
#remove(list = ls())
print(defaultpath)
setwd(defaultpath)
# get all log files
setwd("exp-data")
file_list <- list.files()
show(file_list)
if (exists ("allData")) { rm(allData) }
##we do the analysis once per task
for (task in tasks){
# basic loop that reads over participant files
# and computes normalized errors
print("Working on Task: ")
print(task)
print("-----------------------------------------")
allData <- NULL
for (file in file_list){
if(!grepl(task,file) | grepl("confidence",file) ){
#if the filename doesn't contain the current task short code or the word "confidence_survey" then don't take it
print("Skipping file")
print(file)
next
}
print("Analyzing file")
print(file)
setwd("~/Desktop/OnlineExperiment/AnalysisScripts/exp-data/")
dataFile <- read.csv(file)
# collect all trials in order to calculate error rate, keep non training trials (trial > 0)
tmp <- dataFile
# no training
tmp <- tmp [which(!grepl("practice",tmp$page_name,fixed=TRUE)),]
#convert the error, correct values
#convert the error, correct values
tmp$answer_numeric <- 0
#this way later we will calculate percept correct. If we want "percent wrong" switch the 0 and 1
tmp$answer_numeric[grep("correct",tmp$answer)] <- 1
#combine all files in one big one
if ( !exists("allData") ){
allData <- tmp
} else {
allData <- rbind(allData,tmp)
}
}
print(defaultpath)
setwd(defaultpath)
write.csv(allData, file=paste("~/Desktop/OnlineExperiment/AnalysisScripts/results/all_data-",task,".csv",sep=""))
}
View(tmp)
library(plyr)
library(dplyr)
library(MPDiR)
library(quickpsy)
library(fitdistrplus)
library(ggplot2)
# Global values for our conditions
tasks <- c('T1','T2','T3')
sizes <- c('s1','s2','s3')
# set directory where script is
sourceDir <- dirname (rstudioapi::getActiveDocumentContext()$path)
defaultpath <- sourceDir
#remove(list = ls())
print(defaultpath)
setwd(defaultpath)
# get all log files
setwd("exp-data")
file_list <- list.files()
show(file_list)
if (exists ("allData")) { rm(allData) }
##we do the analysis once per task
for (task in tasks){
# basic loop that reads over participant files
# and computes normalized errors
print("Working on Task: ")
print(task)
print("-----------------------------------------")
allData <- NULL
for (file in file_list){
if(!grepl(task,file) | grepl("confidence",file) ){
#if the filename doesn't contain the current task short code or the word "confidence_survey" then don't take it
print("Skipping file")
print(file)
next
}
print("Analyzing file")
print(file)
setwd("~/Desktop/OnlineExperiment/AnalysisScripts/exp-data/")
dataFile <- read.csv(file)
mydf <- read.table(dataFile)[-c(16), ]
# collect all trials in order to calculate error rate, keep non training trials (trial > 0)
tmp <- mydf
# no training
tmp <- tmp [which(!grepl("practice",tmp$page_name,fixed=TRUE)),]
#convert the error, correct values
tmp$answer_numeric <- 0
#this way later we will calculate percept correct. If we want "percent wrong" switch the 0 and 1
tmp$answer_numeric[grep("correct",tmp$answer)] <- 1
#combine all files in one big one
if ( !exists("allData") ){
allData <- tmp
} else {
allData <- rbind(allData,tmp)
}
}
print(defaultpath)
setwd(defaultpath)
write.csv(allData, file=paste("~/Desktop/OnlineExperiment/AnalysisScripts/results/all_data-",task,".csv",sep=""))
}
library(plyr)
library(dplyr)
library(MPDiR)
library(quickpsy)
library(fitdistrplus)
library(ggplot2)
# Global values for our conditions
tasks <- c('T1','T2','T3')
sizes <- c('s1','s2','s3')
# set directory where script is
sourceDir <- dirname (rstudioapi::getActiveDocumentContext()$path)
defaultpath <- sourceDir
#remove(list = ls())
print(defaultpath)
setwd(defaultpath)
# get all log files
setwd("exp-data")
file_list <- list.files()
show(file_list)
if (exists ("allData")) { rm(allData) }
##we do the analysis once per task
for (task in tasks){
# basic loop that reads over participant files
# and computes normalized errors
print("Working on Task: ")
print(task)
print("-----------------------------------------")
allData <- NULL
for (file in file_list){
if(!grepl(task,file) | grepl("confidence",file) ){
#if the filename doesn't contain the current task short code or the word "confidence_survey" then don't take it
print("Skipping file")
print(file)
next
}
print("Analyzing file")
print(file)
setwd("~/Desktop/OnlineExperiment/AnalysisScripts/exp-data/")
dataFile <- read.csv(file)
print(dataFile)
# mydf <- read.table(dataFile)[-c(16), ]
# collect all trials in order to calculate error rate, keep non training trials (trial > 0)
tmp <- mydf
# no training
tmp <- tmp [which(!grepl("practice",tmp$page_name,fixed=TRUE)),]
#convert the error, correct values
tmp$answer_numeric <- 0
#this way later we will calculate percept correct. If we want "percent wrong" switch the 0 and 1
tmp$answer_numeric[grep("correct",tmp$answer)] <- 1
#combine all files in one big one
if ( !exists("allData") ){
allData <- tmp
} else {
allData <- rbind(allData,tmp)
}
}
print(defaultpath)
setwd(defaultpath)
write.csv(allData, file=paste("~/Desktop/OnlineExperiment/AnalysisScripts/results/all_data-",task,".csv",sep=""))
}
library(plyr)
library(dplyr)
library(MPDiR)
library(quickpsy)
library(fitdistrplus)
library(ggplot2)
# Global values for our conditions
tasks <- c('T1','T2','T3')
sizes <- c('s1','s2','s3')
# set directory where script is
sourceDir <- dirname (rstudioapi::getActiveDocumentContext()$path)
defaultpath <- sourceDir
#remove(list = ls())
print(defaultpath)
setwd(defaultpath)
# get all log files
setwd("exp-data")
file_list <- list.files()
show(file_list)
if (exists ("allData")) { rm(allData) }
##we do the analysis once per task
for (task in tasks){
# basic loop that reads over participant files
# and computes normalized errors
print("Working on Task: ")
print(task)
print("-----------------------------------------")
allData <- NULL
for (file in file_list){
if(!grepl(task,file) | grepl("confidence",file) ){
#if the filename doesn't contain the current task short code or the word "confidence_survey" then don't take it
print("Skipping file")
print(file)
next
}
print("Analyzing file")
print(file)
setwd("~/Desktop/OnlineExperiment/AnalysisScripts/exp-data/")
dataFile <- read.csv(file)
# print(dataFile)
mydf <- read.table(dataFile)[-c(16), ]
# collect all trials in order to calculate error rate, keep non training trials (trial > 0)
tmp <- mydf
# no training
tmp <- tmp [which(!grepl("practice",tmp$page_name,fixed=TRUE)),]
#convert the error, correct values
tmp$answer_numeric <- 0
#this way later we will calculate percept correct. If we want "percent wrong" switch the 0 and 1
tmp$answer_numeric[grep("correct",tmp$answer)] <- 1
#combine all files in one big one
if ( !exists("allData") ){
allData <- tmp
} else {
allData <- rbind(allData,tmp)
}
}
print(defaultpath)
setwd(defaultpath)
write.csv(allData, file=paste("~/Desktop/OnlineExperiment/AnalysisScripts/results/all_data-",task,".csv",sep=""))
}
library(plyr)
library(dplyr)
library(MPDiR)
library(quickpsy)
library(fitdistrplus)
library(ggplot2)
# Global values for our conditions
tasks <- c('T1','T2','T3')
sizes <- c('s1','s2','s3')
# set directory where script is
sourceDir <- dirname (rstudioapi::getActiveDocumentContext()$path)
defaultpath <- sourceDir
#remove(list = ls())
print(defaultpath)
setwd(defaultpath)
# get all log files
setwd("exp-data")
file_list <- list.files()
show(file_list)
if (exists ("allData")) { rm(allData) }
##we do the analysis once per task
for (task in tasks){
# basic loop that reads over participant files
# and computes normalized errors
print("Working on Task: ")
print(task)
print("-----------------------------------------")
allData <- NULL
for (file in file_list){
if(!grepl(task,file) | grepl("confidence",file) ){
#if the filename doesn't contain the current task short code or the word "confidence_survey" then don't take it
print("Skipping file")
print(file)
next
}
print("Analyzing file")
print(file)
setwd("~/Desktop/OnlineExperiment/AnalysisScripts/exp-data/")
dataFile <- read.csv(file)
class(dataFile)
# print(dataFile)
# mydf <- read.table(dataFile)[-c(16), ]
# collect all trials in order to calculate error rate, keep non training trials (trial > 0)
tmp <- mydf
# no training
tmp <- tmp [which(!grepl("practice",tmp$page_name,fixed=TRUE)),]
#convert the error, correct values
tmp$answer_numeric <- 0
#this way later we will calculate percept correct. If we want "percent wrong" switch the 0 and 1
tmp$answer_numeric[grep("correct",tmp$answer)] <- 1
#combine all files in one big one
if ( !exists("allData") ){
allData <- tmp
} else {
allData <- rbind(allData,tmp)
}
}
print(defaultpath)
setwd(defaultpath)
write.csv(allData, file=paste("~/Desktop/OnlineExperiment/AnalysisScripts/results/all_data-",task,".csv",sep=""))
}
library(plyr)
library(dplyr)
library(MPDiR)
library(quickpsy)
library(fitdistrplus)
library(ggplot2)
# Global values for our conditions
tasks <- c('T1','T2','T3')
sizes <- c('s1','s2','s3')
# set directory where script is
sourceDir <- dirname (rstudioapi::getActiveDocumentContext()$path)
defaultpath <- sourceDir
#remove(list = ls())
print(defaultpath)
setwd(defaultpath)
# get all log files
setwd("exp-data")
file_list <- list.files()
show(file_list)
if (exists ("allData")) { rm(allData) }
##we do the analysis once per task
for (task in tasks){
# basic loop that reads over participant files
# and computes normalized errors
print("Working on Task: ")
print(task)
print("-----------------------------------------")
allData <- NULL
for (file in file_list){
if(!grepl(task,file) | grepl("confidence",file) ){
#if the filename doesn't contain the current task short code or the word "confidence_survey" then don't take it
print("Skipping file")
print(file)
next
}
print("Analyzing file")
print(file)
setwd("~/Desktop/OnlineExperiment/AnalysisScripts/exp-data/")
dataFile <- read.csv(file)
class(dataFile)
# print(dataFile)
# mydf <- read.table(dataFile)[-c(16), ]
# collect all trials in order to calculate error rate, keep non training trials (trial > 0)
tmp <- dataFile
# no training
tmp <- tmp [which(!grepl("practice",tmp$page_name,fixed=TRUE)),]
#convert the error, correct values
tmp$answer_numeric <- 0
#this way later we will calculate percept correct. If we want "percent wrong" switch the 0 and 1
tmp$answer_numeric[grep("correct",tmp$answer)] <- 1
#combine all files in one big one
if ( !exists("allData") ){
allData <- tmp
} else {
allData <- rbind(allData,tmp)
}
}
print(defaultpath)
setwd(defaultpath)
write.csv(allData, file=paste("~/Desktop/OnlineExperiment/AnalysisScripts/results/all_data-",task,".csv",sep=""))
}
